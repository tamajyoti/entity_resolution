# AUTOGENERATED! DO NOT EDIT! File to edit: 00_utils.ipynb (unless otherwise specified).

# Cell
import json
from datetime import datetime, timedelta
from hashlib import sha224
from typing import List, Optional

import pandas as pd
import textdistance as tdist
from dfply import *
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search, A, Q
from pymongo import MongoClient

VALID_INDEX_TYPE = {"articles", "mentions"}


# Cell

def read_config(json_path: str = "config.json") -> dict:
    """ Read json config file

    Parameters
    ----------
    json_path: str, optional
        Path to the project json config file. Default is to use the one found at root directory

    Returns
    -------
    dict
        with the data contained in the config file.
    """
    with open(json_path) as json_file:
        config = json.load(json_file)
        try:
            config["taxonomy_mapping"]["listing_subtypes"] = pd.DataFrame(
                config["taxonomy_mapping"]["listing_subtypes"]).drop_duplicates()
            config["taxonomy_mapping"]["listing_subclasses"] = pd.DataFrame(
                config["taxonomy_mapping"]["listing_subclasses"]).drop_duplicates()
        except:
            pass

    return (config)


def recursive_get(d: dict, *args, default=None):
    """Recursive implementation of dictionary value extraction

    Parameters
    ----------
    d: dict
        dictionary
    *args: dict
        path to the field required in `d`
    default: dict
        value to be returned if nothing is found. Default to `None


    Returns
    -------
    dict
        value found at the given path provided in `*args`

    """
    if d is None:
        return default

    if not args:
        return d
    key, *args = args
    return recursive_get(d.get(key, default), *args, default=default)


def encode(input_str: str) -> str:
    """Enforce `input` encoding to utf-8

    Parameters
    ----------
    input_str: str
        input string to encode

    Returns
    -------
    str
        Same as `input` string with utf-8 encoding enforced

    """
    try:
        input_str = input_str.encode('utf-8')
    except UnicodeError:
        input_str = re.sub(r"\W", '', input_str).encode('utf-8')

    return input_str


def snippet_distance(snippet_1: str, snippet_2: str) -> float:
    """Compute distance between 2 snippest

    We first encode the 2 snippests then we take the average of different text distances
    provided by the `textdistance` package

    Parameters
    ----------
    snippet1, snippet2 : str
        Text snippet

    Returns
    -------
    float
        The similarity score
    """

    _snippet_1 = encode(snippet_1)
    _snippet_2 = encode(snippet_2)
    _score = tdist.hamming.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.damerau_levenshtein.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.jaro.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.smith_waterman.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.jaccard.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.bag.normalized_similarity(_snippet_1, _snippet_2) + \
             tdist.lcsstr.normalized_similarity(_snippet_1, _snippet_2)
    return _score / 7.0


# Cell

def mongo_connect(config: dict, host_name: str = "staging") -> MongoClient:
    """
    Establish connection to the project's mongoDB

    Parameters
    ----------
    config : dict
        Project config dictionary
    host_name : str, optional
        Host name (should be defined in config)

    Returns
    -------
    MongoClient
        MongoClient to the project mongod instance
    """

    return MongoClient(recursive_get(config, "mongo", "host", host_name))


# Cell

def _chunker(seq, size):
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))


def get_entity_ids(mongo_client: MongoClient, config: dict,
                   mention_ids: List) -> pd.DataFrame:
    """
    Extract Entity ids for given mentions

    Parameters
    ----------
    mongo_client : MongoClient
        mongo connection client
    config : dict
        Project config dictionary
    mention_ids : List
       mention ids

    Returns
    -------
    pandas.DataFrame
        DataFrame with 2 columns `mention_db_id` (the provided mentions) and the associated `entity_id`
    """
    db = mongo_client[recursive_get(
        config, "mongo", "mention_factory", "db_name")]
    mapping = db[recursive_get(config, "mongo", "mention_factory", "mapping")]
    output = []
    fields = {"mention": 1, "entity_id": 1, "_id": 0}

    # send query by package of 100k to avoid the 16MB limit from mongoDB
    for mention_group in _chunker(mention_ids, 100000):
        query = {"mention": {"$in": mention_group}}
        result = list(mapping.find(query, fields))
        if len(result):
            res = pd.DataFrame(list(mapping.find(query, fields))) >> \
                  rename(mention_db_id=X.mention)
        else:
            res = pd.DataFrame(list(zip(mention_group,
                                        [None] * len(mention_group))), columns=["mention_db_id", "entity_id"]
                               )

        output.append(res.drop_duplicates(["mention_db_id"]))

    return pd.concat(output).drop_duplicates(["mention_db_id"])


# Cell

def _extract_binary_prob(doc: dict, default_proba_val: float = 0) -> pd.DataFrame:
    # binary model version
    try:
        binary_model_version = recursive_get(
            doc, "binary_ml_version", default=None)
        if binary_model_version is None:
            binary_model_version = recursive_get(doc, "version_info", "binary_model",
                                                 default="regexp")
    except:
        binary_model_version = "regexp"

    # extract positive probability
    try:
        positive_proba = recursive_get(doc, "probability", "binary", "positive",
                                       default=default_proba_val)
    except:
        positive_proba = default_proba_val

    return pd.DataFrame({"label": doc["listing_type"],
                         "probability": positive_proba,
                         "model_version": binary_model_version},
                        index=[0])


def _extract_multiclass_proba(doc: dict,
                              class_conv_tab: pd.DataFrame,
                              subtype_conv_tab: pd.DataFrame,
                              default_proba_val: float = 0) -> dict:
    # prepare conversion table
    amt_v2_table = class_conv_tab >> select(
        X.amt_v2) >> rename(subclass=X.amt_v2)
    amt_v2_table = amt_v2_table.drop_duplicates()

    # model version
    try:
        model_version = recursive_get(
            doc, "multiclass_ml_version", default=None)
        if model_version is None:
            model_version = recursive_get(doc, "version_info", "multiclass_model",
                                          default="Unknown")
    except:
        model_version = "Unknown"

    # listing subtype
    listing_subtype = [x["subtype"] for x in doc["listing_subtypes"]]
    # convert v1 to v2 if needed
    is_v2_subtype = any(["adverse-media-v2" in i for i in listing_subtype])
    if not is_v2_subtype:
        _subtype = pd.DataFrame(listing_subtype, columns=["amt_v1"]) >> \
                   left_join(subtype_conv_tab, by="amt_v1")
        listing_subtype = _subtype["amt_v2"].tolist()

    # extract all multi-class probs
    try:
        _multi_class = doc["probability"]["multiclass"]
        multi_class_df = pd.DataFrame({"subclass": [*_multi_class.keys()],
                                       "probability": [*_multi_class.values()]
                                       })

        # check if v2 taxonomy exists
        is_amt_v2 = any(["adverse-media-v2" in i for i in _multi_class.keys()])

        if is_amt_v2:
            # cover the case where v2 and v1 taxonomy are used
            multi_class_df = multi_class_df.loc[["adverse-media-v2" in i
                                                 for i in multi_class_df.subclass]
            ]
        else:
            # convert to v2 taxonomy
            multi_class_df >>= rename(amt_v1=X.subclass) >> left_join(
                class_conv_tab, by="amt_v1")
            multi_class_df >>= rename(subclass=X.amt_v2) >> group_by(X.subclass) >> \
                               summarize(probability=X.probability.sum())
            multi_class_df = amt_v2_table >> left_join(
                multi_class_df, by="subclass")
            multi_class_df = multi_class_df.fillna(0)

        # update listing subtype: keep only the one with highest prob
        v2_subtype = [
            multi_class_df.iloc[multi_class_df.probability.argmax()]["subclass"]]
        listing_subtype = [
            value for value in v2_subtype if value in listing_subtype]

    except (KeyError, TypeError):
        multi_class_df = amt_v2_table >> mutate(probability=default_proba_val)

    return {"model_version": model_version,
            "listing_subtype": listing_subtype,
            "multi_class_df": multi_class_df.reset_index()}


def get_mention_info(doc_id: str, mongo_client: MongoClient, config: dict,
                     default_binary_proba: float = 0, default_mc_proba: float = 0) -> dict:
    """
    Extract mention info

    For a given `doc_id` uniquely identifying a mention in the database,
    we extract the required info, deal with missing values and format them properly.

    Parameters
    ----------
    doc_id : str
        the document identifier in the database corresponding to
        the `_id` field in the `mention` collection
    mongo_client : MongoClient
        mongo connection client
    config : dict
        Project config dictionary
    default_binary_proba, default_mc_proba : float
        Default probability to apply in the binary or multi-class case.
        Should be a number between 0 and 1.

    Returns
    -------
    dict
    """
    db = mongo_client[recursive_get(
        config, "mongo", "mention_factory", "db_name")]
    mention = db[recursive_get(config, "mongo", "mention_factory", "mention")]
    query = {"_id": doc_id}

    # we know there is a unique document per doc_id
    doc = mention.find_one(query)

    # format binary proba
    doc.update({"listing_df": _extract_binary_prob(doc, default_binary_proba)})

    # format multi-class proba
    _class_conv_tab = recursive_get(
        config, "taxonomy_mapping", "listing_subclasses")
    _subtype_conv_tab = recursive_get(
        config, "taxonomy_mapping", "listing_subtypes")
    doc.update({"sublisting_df": _extract_multiclass_proba(doc,
                                                           _class_conv_tab,
                                                           _subtype_conv_tab,
                                                           default_mc_proba)
                })

    return doc


def get_all_entities_info_mdb(config: dict, mongo_client: MongoClient,
                              db_name: str = "mentionfactory",
                              collection_name: str = "mentions",
                              date_field_name: str = "extracted_date",
                              date: Optional[str] = "",
                              fields: Optional[List] = [],
                              chunker_size: Optional[int] = 5000) -> pd.DataFrame:
    mongo_con = mongo_client[db_name][collection_name]
    query = {}
    qout = {}
    if date:
        start_date = datetime.strptime(
            date + "T00:00:05Z", "%Y-%m-%dT%H:%M:%SZ")
        end_date = datetime.strptime(date + "T23:59:59Z", "%Y-%m-%dT%H:%M:%SZ")
        query = {date_field_name: {"$lte": end_date, "$gte": start_date}}

    if fields:
        qout = {k: 1 for k in fields}

    _mentions = mongo_con.find(query, qout)
    _mentions_df = pd.DataFrame([doc for doc in _mentions])
    if _mentions_df.shape[0]:
        all_ids = _mentions_df["_id"].tolist()
        entity_ids = pd.concat([get_entity_ids(mongo_client_local, config, _ids)
                                for _ids in _chunker(all_ids, chunker_size)]
                               )
        _mentions_df >>= rename(mention_db_id=X["_id"]) >> \
                         left_join(entity_ids, by="mention_db_id") >> \
                         arrange(X.entity_id)

    return _mentions_df


# Cell

def es_connect(config: dict, host_name: str = "prod", add_index: bool = True) -> Elasticsearch:
    """
    Establish connection to the project's elasticsearch

    Parameters
    ----------
    config : dict
        Project config dictionary
    host_name : str
        name of the host to connect to; should be defined in config

    Returns
    -------
    ElasticSearch Client
         ElasticSearch Client to the project es instance
    """
    es_config = config["elastic_search"]
    es_client = Elasticsearch(
        es_config["host"][host_name], timeout=es_config["time_out"])
    if add_index:
        es_client.indices.put_settings(
            body={"index": {"max_result_window": 500000}})
    return es_client


# Cell

def get_document_es(es_client: Elasticsearch, config: dict,
                    query: dict,
                    fields: Optional[List] = [],
                    index_type: str = "articles",
                    date_field_name: str = "tstamp",
                    date: Optional[str] = "",
                    start_date: Optional[str] = "", end_date: Optional[str] = "") -> List[dict]:
    """
    Gives one document either article or mentions based on id from elastic search

    Parameters
    ----------
    es_client: Elasticsearch
    config : dict
        Project config dictionary
    query : dict
        Takes the exact id of the document which needs to be fetched, it can
        be any other unique key in the document
    fields : List, optional
        Fields we want to be returned
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`.
    date : str, optional
        if provided, search would be restricted for the given date.
        Date format is expected to be in %YYYY-mm-dd

    Returns
    -------
    List of dict
        List of dictionaries where each dictionary represents the info returned by one document
    """
    # checking inputs
    if index_type not in VALID_INDEX_TYPE:
        raise ValueError("index_type must be one of %s." % VALID_INDEX_TYPE)

    # select wich index to connect to
    es_index = config["elastic_search"]["indexes"][index_type]

    s = Search(using=es_client, index=es_index)
    if query:
        keys = list(query.keys())[0]
        val = list(query.values())[0]
        if not isinstance(val, list):
            val = [val]
        s = s.filter("terms", **{keys: val})

    if fields:
        s = s.source(fields)

    # add date restriction if needed
    if start_date and end_date:
        start_date = start_date + "T00:00:00"
        end_date = end_date + "T23:59:59"
        s = s.query(
            "range", **{date_field_name: {"gte": start_date, "lte": end_date}})
    elif date:
        # check date is in the right format
        start_date = date + "T00:00:00"
        end_date = date + "T23:59:59"
        s = s.query(
            "range", **{date_field_name: {"gte": start_date, "lte": end_date}})

    # extract info
    count = s.count()
    response = s[0:count].execute()

    # populate the results
    results = []
    for hit in response:
        dico = {key: hit[key] for key in hit}
        dico.update({"_id": hit.meta.to_dict()["id"]})
        results.append(dico)

    return results


# Cell
def get_all_entities_info_es(es_client: Elasticsearch, config: dict, mongo_client: MongoClient,
                             fields: Optional[List] = [], query={},
                             index_type: str = "mentions",
                             date_field_name: str = "extracted_date",
                             date: str = "2019-02-03") -> pd.DataFrame:
    """Extract all infos for entities and the associated mentions

    Parameters
    ----------
    es_client: Elasticsearch
    config: dict
        Project config dictionary
    mongo_client: MongoClient
        mongo client used mainly to extract entity ids
    fields : List, optional
        Fields we want to be returned
    query : dict
        Takes the exact id of the document which needs to be fetched, it can
        be any other unique key in the document
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`.
    date_field_name: str
        name of the date field
    date : str, optional
        if provided, search would be restricted for the given date.
        Date format is expected to be in %YYYY-mm-dd

    Returns
    -------
    pd.DataFrame
        one column per field given in 'fields' and as many rows as mentions found
    """
    if "uri" not in fields:
        fields.append("uri")
    if "entity_name" not in fields:
        fields.append("entity_name")

    entity_example = pd.DataFrame(get_document_es(es_client=es_client,
                                                  config=config,
                                                  query=query,
                                                  fields=fields,
                                                  index_type=index_type,
                                                  date_field_name=date_field_name,
                                                  date=date))
    if entity_example.shape[0]:
        entity_ids = get_entity_ids(
            mongo_client, config, entity_example["_id"].tolist())
        entity_example >>= rename(mention_db_id=X["_id"]) >> \
                           left_join(entity_ids, by="mention_db_id") >> \
                           arrange(X.entity_id)
    return entity_example


# Cell

def check_article_extraction(uri: str, es_client: Elasticsearch, config: dict,
                             date_range: List[str], flexibility_days: int = 1,
                             index_type: str = "articles",
                             date_field_name: str = "tstamp",
                             ) -> dict:
    """Check if an article (defined by an `uri`) has been extracted in a given time window

    Given an `uri` defining an article, we would like to check if the article was extracted
    in a given date range (defined by `date_range`).

    Parameters
    ----------
    uri: str
        the uri to be checked
    es_client: Elasticsearch
        elastic search client
    date_range: List[str]
        dates to checked in YYYY-mm-dd format
    flexibility_days: int
        how many days around the dates passed in `date_range` should we look at.
        Basically, for every date found in `date_range`, we will look for an aricle extracted
        between day - `flexibility_days`, `day + flexibility_days`.
    config: dict
        Project config dictionary
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`.
    date_field_name: str
        name of the date field

    Returns
    -------
    dict
        `uri`: the `uri` passed
        `exist_in_date_range`: an article was extracted as one of the passed dates in `date_range`
        `exists_in_date_neighberhood`: an article was extracted at date in the neighbehood of
        one of the date found in `date_range`

    """
    # create list of ranges we care about
    start_date = [(pd.to_datetime(date_str) - timedelta(days=flexibility_days)).strftime("%Y-%m-%d")
                  for date_str in date_range]
    end_date = [(pd.to_datetime(date_str) + timedelta(days=flexibility_days)).strftime("%Y-%m-%d")
                for date_str in date_range]

    _exist_in_date_range = False
    _exists_in_date_neighberhood = False
    for i in range(len(date_range)):
        article_dates = pd.DataFrame(get_document_es(es_client=es_client,
                                                     config=config,
                                                     query={"url": uri},
                                                     fields=[
                                                         "url", date_field_name],
                                                     index_type=index_type,
                                                     date_field_name=date_field_name,
                                                     start_date=start_date[i],
                                                     end_date=end_date[i])
                                     )
        _exists_in_date_neighberhood = article_dates.shape[0] > 0
        if _exists_in_date_neighberhood:
            article_dates["date"] = pd.to_datetime(
                article_dates[date_field_name]).dt.strftime("%Y-%m-%d")
            _exist_in_date_range = article_dates[article_dates["date"].isin(
                [date_range[i]])].shape[0] > 0

        if _exist_in_date_range or _exists_in_date_neighberhood:
            break

    return {"exist_in_date_range": _exist_in_date_range,
            "exists_in_date_neighberhood": _exists_in_date_neighberhood,
            "uri": uri
            }


# Cell

def keep_article(article_uri: str, es_client: Elasticsearch, config: dict,
                 index_type: str = "articles", dates_check: Optional[List] = []) -> dict:
    """Keep an article if the `article_uri` has a uniuqe content in the database

    Parameters
    ----------
    article_uri : str
        The article uri
    es_client: Elasticsearch
    config : dict
        Project config dictionary
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`.
    dates_check: List[str], optional
        dates to check in YYYY-mm-dd format. If not empty, the test is to check that the url
        has only one occurence in the provided list


    Returns
    -------
    dict
        keep: Wether the article should be kept or not
        info: Info extratced to make the `keep` decision


    """
    try:
        _results = get_document_es(es_client=es_client,
                                   config=config,
                                   query={"url": article_uri},
                                   fields=["title", "tstamp"],
                                   index_type=index_type)
        _article_raw = pd.DataFrame(_results)
        _article_raw["tstamp_str"] = pd.to_datetime(
            _article_raw.tstamp).dt.strftime("%Y-%m-%d")

        _article = _article_raw.copy()
        if dates_check:
            _article = _article[_article["tstamp_str"].isin(dates_check)]

        _article = _article.drop_duplicates(["title", "tstamp"])
    except:
        _article = pd.DataFrame()
    return {"keep": _article.shape[0] == 1,
            "info": _article,
            "full_data": _article_raw
            }


def create_article_id(article_uri: str, article_title: str) -> str:
    """Create an article unique ID

    After encoding the inputs to utf8, the function concatenates them and
    calls `sha224()` to create the key.

    Parameters
    ----------
    article_uri : str
        The article uri
    article_title : str
        The article title

    Returns
    -------
    str
        Unique key produced identifying the article.

    """
    if article_title is None:
        article_title = ""
    unique_string = encode(article_uri) + encode(article_title)
    return sha224(unique_string).hexdigest()


# Cell
def create_mention_id(article_url, snippet, entity_name):
    """Create a (formatted) mention unique ID.

    Parameters
    ----------
    article_url : str
        The article url
    entity_name : str
        The entity name
    snippet : str
        The mention snippet

    Returns
    -------
    str
        Unique key produced identifying a mention.
    """
    unique_string = encode(article_url) + encode(snippet) + encode(entity_name)
    return sha224(unique_string).hexdigest()


# Cell

def es_document_summary_stats(es_client: Elasticsearch, config: dict,
                              start_date: str, end_date: str,
                              date_field_name: str = "extracted_date",
                              field_name: str = "language",
                              field_val: Optional[List] = [],
                              aggreg_field_name: Optional[str] = None,
                              index_type: str = "articles") -> pd.DataFrame:
    """
    Gives summary statistics based on time range and field of choice for either mentions or articles

    Parameters
    ----------
    es_client: Elasticsearch
    config : dict
        Project config dictionary
    date_field_name : str
        Takes the exact name of the date field on which range needs to be applied to. Default
        to "extracted_date"
    start_date : str ; end_date : str
        Takes the start and end date to determine the timeline to fetch articles in YYYY-mm-dd format
    field_name : str
        The field on which the grouping has to be done, Default is language for nutch-* (mentions) index
    field_val : list, optional
        The list of values of the particular field that we will be filtering on
    aggreg_field_name: str, optional
        The field name we will make an aggregation on
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`.

    Returns
    -------
    pandas.DataFrame
         Returns a pandas.DataFrame with 2 columns: `field_name` and `count`.
    """
    # checking inputs
    if index_type not in VALID_INDEX_TYPE:
        raise ValueError("index_type must be one of %s." % VALID_INDEX_TYPE)

    if datetime.strptime(start_date, "%Y-%m-%d") >= datetime.strptime(end_date, "%Y-%m-%d"):
        raise ValueError(
            "start_date: %s should be earlier to end_date: %s" % (start_date, end_date))

    # select wich index to connect to
    es_index = config["elastic_search"]["indexes"][index_type]

    # initialise the result DataFrame
    df_results = pd.DataFrame()

    # loop over field_val and send one request per val
    for val in field_val:
        s = Search(using=es_client, index=es_index) \
            .query("range", **{date_field_name: {"gte": start_date, "lt": end_date}}) \
            .filter("match", **{field_name: val})

        if aggreg_field_name is None:
            _stats = pd.DataFrame({"field_name": [val], "count": s.count()})
        else:
            # add aggregation if needed
            _total_all = s.count()
            a = A('terms', field=aggreg_field_name, size=999999)
            bucket_name = "per_" + aggreg_field_name
            s.aggs.bucket(bucket_name, a)

            # execute response
            response = s.execute()

            # collect results in a DataFrame
            _stats = pd.DataFrame({aggreg_field_name: domain_result["key"],
                                   "count": domain_result["doc_count"]}
                                  for domain_result in response.aggregations[bucket_name]
                                  )
            if _stats.shape[0]:
                _stats = _stats.append(pd.DataFrame({aggreg_field_name: None,
                                                     "count": _total_all - _stats["count"].sum()
                                                     }, index=[0]))

                _stats = _stats >> mutate(prob=X["count"] / _stats["count"].sum(),
                                          field_name=val)

                df_results = df_results.append(_stats)

    return df_results.reset_index(drop=True) \
        .rename(columns={"field_name": field_name})


# Cell

def es_article_sampling(n: int, es_client: Elasticsearch, config: dict,
                        summary_stats: pd.DataFrame,
                        start_date: str, end_date: str,
                        date_field_name: str = "extracted_date",
                        lang_field_name: str = "locale",
                        index_type: str = "articles",
                        rm_dup_field_name: str = "uri",
                        fields: Optional[List] = [],
                        dates_check: Optional[List[str]] = []) -> pd.DataFrame:
    """
    Gives a list of articles sampled from some previously computed distribution.
    A couple of assumptions are made here:
    1) the first column in `summary_stats` is the field we want to use as key for sampling
    2) A prob column has been created

    Parameters
    ----------
    n : int
        The total number of documents to return
    es_client: Elasticsearch
    config : dict
        Project config dictionary
    summary_stats : (pandas) DataFrame
        Distribution of documents over a term and time range
    start_date, end_date : str
        Takes the start and end date to determine the timeline to fetch articles
    date_field_name: str
        The name of the date field
    lang_field_name: str
        The language field name. Useful in case some sources have articles in different languages
    rm_dup_field_name: str
        Duplicate on this field will be dropped
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`
    fields : List, optional
        Fields we want to be returned
    dates_check: List[str], optional
        List of dates in YYYY-mm-dd format used to decide if we keep an article of not.
        It is required that only one article was extratced during these dates
        for an article to be analysed.


    Returns
    -------
    sample_docs : pd.DataFrame
        A DataFrame with as many columns as elements in fields and one row per article uri

    """
    # checking inputs
    if index_type not in VALID_INDEX_TYPE:
        raise ValueError("index_type must be one of %s." % VALID_INDEX_TYPE)

    # es index
    es_index = config["elastic_search"]["indexes"][index_type]

    # Initialize the global document list
    sample_docs = []

    # Distribute no of documents based on summary stats
    summary_stats["n_doc"] = round(summary_stats["prob"] * n, 0)
    summary_stats >>= mask(X.n_doc >= 0)
    field_name = summary_stats.columns[0]
    lang_val = summary_stats[lang_field_name].unique().tolist()[0]

    for j, val in enumerate(summary_stats[field_name].unique()):
        n_doc = int(summary_stats.n_doc.tolist()[j])
        # prepare the query
        s = Search(using=es_client, index=es_index) \
            .query("range", **{date_field_name: {"gte": start_date, "lt": end_date}}) \
            .filter("match", **{lang_field_name: lang_val})
        if val is None:
            s = s.filter(~Q('exists', field=field_name))
        else:
            s = s.filter("match", **{field_name: val})

        if fields:
            if "uri" not in fields:
                fields.append("uri")
            s = s.source(fields)

        # get first n_doc
        s = s[0:n_doc]

        # executre it; an iterator will be returned
        response = s.execute()
        for sample in response:
            sample_df = pd.DataFrame([{key: sample[key] for key in fields}])
            sample_df["start_date"] = start_date
            sample_docs.append(sample_df)

    return pd.concat(sample_docs).drop_duplicates([rm_dup_field_name])
