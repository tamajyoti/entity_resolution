# AUTOGENERATED! DO NOT EDIT! File to edit: 02_name_commonness.ipynb (unless otherwise specified).

__all__ = ["save_mongo_entity_to_csv", "save_entity_tokens_to_csv", "get_commonness_score"]

import csv
from collections import Counter
from functools import reduce
from typing import Dict, Optional

# Cell
import unidecode
from dfply import *
from pymongo import MongoClient

from .config import recursive_get, read_mongo


# Cell
def save_mongo_entity_to_csv(mongo_client: MongoClient, config: Dict, output_filename: str) -> None:
    """
    Creates and updates a CSV with all the unique entity names present in the mongo db
    based on the created mongo cursor.

    Parameters:
    --------------------------------------------------------------------
    mongo_client: MongoClient
        mongo connection to the desired cluster
    config: Dict
        project config dictionary (usually read from json)
    output_filename: str,
        The csv where the file needs to be updated

    Returns:
    --------------------------------------------------------------------
    None
    """
    db = mongo_client[recursive_get(config, "mongo", "mention_factory", "db_name")]
    metadata_cursor = db[recursive_get(config, "mongo", "mention_factory", "entity")].find(
        {}, {"name": 1}
    )

    with open(output_filename, "w") as f1:
        writer = csv.writer(
            f1,
            delimiter="\t",
            lineterminator="\n",
        )
        writer.writerow({"names_list"})
        for metadata in metadata_cursor:
            row = metadata["name"]
            writer.writerow([row])


def save_entity_tokens_to_csv(names_csv: str, output_filename: str) -> None:
    """
    The function reads all the entity names obtained from mongo and then transforms them
    into individual tokens and then count the tokens. Finally it returns all the name tokens
    and their individual counts which are stored in a csv file

    Parameters:
    -----------
    names_csv: str,
        The csv file containing all the unique entity names from mongo db
    output_filename: str,
        The csv file where all the entity name tokens and their total count is saved

    Returns:
    ---------
    None
    """
    df_all_names = pd.read_csv(names_csv)
    df_all_names["names_split"] = df_all_names["names_list"].apply(
        lambda row: [unidecode.unidecode(x.lower()) for x in list(row.split(" "))]
    )

    flat_list = [item for sublist in df_all_names.names_split for item in sublist]
    names_tokens = list(Counter(flat_list).items())
    df_names_freq = pd.DataFrame(names_tokens)

    df_names_freq.columns = ["word_token", "no_of_tokens"]
    df_names_freq.to_csv(output_filename)


# Cell
def get_commonness_score(
    entity_name: str,
    config: Dict,
    mongo_client: MongoClient,
    df_names_freq: Optional[pd.DataFrame] = None,
    total_names: Optional[int] = None,
) -> float:
    """
    Gives name commonness score for an entity name

    Parameters:
    ----------
    entity_name: str,
        The entity name we want to get the score for
    config: Dict
        project config dictionary (usually read from json)
    df_name_freq: pd.DataFrame, optional
        The data frame containing all the name tokens and their count
    total_name: int, optional
        The total no of names in our database

    Returns
    -------
    score: A name rank score for the entity name
    """
    # split the name into tokens
    name_token = [y.lower() for y in entity_name.split()]

    if (df_names_freq is None) or (total_names is None):
        # extract data from mongo
        db_name = recursive_get(config, "mongo", "am_combiner", "db_name")
        name_list_col = recursive_get(config, "mongo", "am_combiner", "name_list")
        name_token_col = recursive_get(config, "mongo", "am_combiner", "name_tokens")
        total_names = mongo_client[db_name][name_list_col].count_documents({})
        df_temp = read_mongo(
            mongo_client, db_name, name_token_col, {"word_token": {"$in": name_token}}
        )
    else:
        # get the count of the names tokens from pre saved database files
        df_temp = df_names_freq[df_names_freq["word_token"].isin(name_token)]

    if len(df_temp) < len(name_token):
        common_rank = 0
    else:
        df_temp >>= mutate(perc=df_temp["no_of_tokens"] / total_names)
        # check the percentage of names containing the tokens of the given name and multiplies the percentage for each token
        common_rank = reduce(lambda x, y: x * y, df_temp.perc)

    return common_rank
