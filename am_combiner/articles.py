# AUTOGENERATED! DO NOT EDIT! File to edit: 01_articles_data.ipynb (unless otherwise specified).

__all__ = ['get_entity_info', 'save_article_info', 'save_entity_data_from_es', 'save_entity_data_from_ae',
           'from_local_to_mongo', 'from_mongo_to_article_df']

import json
# Cell
import os
import random
from datetime import timedelta
from typing import List, Optional, Tuple

import pandas as pd
import requests
from dfply import *
from elasticsearch import Elasticsearch
from pymongo import MongoClient
from tqdm import tqdm

from .config import recursive_get, read_mongo, send_df_to_mongo
from .name_commonness import get_commonness_score
from .utils.ab_utils import get_all_entities_info_es, get_document_es, create_article_id


# Cell
def _clean_nested_lists(input: List) -> List:
    """
    Utility function to clean nested lists

    Parameters
    ----------
    input : List
        nested list to clean up

    Returns
    -------
    List
        unnested list

    """
    output = []

    def _remove_nestings(l):
        for i in l:
            if type(i) == list:
                _remove_nestings(i)
            else:
                output.append(i)
    _remove_nestings(input)
    return output

def get_entity_info(start_date: str, end_date: str,
                    es_client: Elasticsearch, config: dict, mongo_client: MongoClient,
                    df_names_freq: pd.DataFrame, total_names: int,
                    locale: str = "en", desired_total_entities: int = 1000,
                    min_name_com_score: float = 1e-7, aggreg_freq: int = 15,
                    entity_info_path: str = ".", nbr_articles_desired: int = 100) -> pd.DataFrame:
    """
    Extract entity information for a given range of time

    Parameters
    ----------
    start_date, end_date : str
        start and end date in YYYY-mm-dd format
    es_client : Elasticsearch
        elastic search client
    config : dict
        Project config dictionary
    mongo_client: MongoClient
        mongo-client to the cluster from where we want to extract the data
    df_names_freq : pandas.DataFramce
        DataFrame needed to compute name commenness score
    total_names : int
        total number of names used to compute name commenness
    locale : str
        name of the language used for filtering the entity to be extracted
    desired_total_entities : int
        total number of entities required for extraction
    min_name_com_score: float
        minimin name commenness score to keep an entity
    aggreg_freq : int
        how often do we need to aggregate the daily info extracted and check if we fulfill the extraction condition
    entity_info_path : str
        path to where the daily entity extraction is saved or read from
    nbr_articles_desired : int
        minimum number of articles (identified by unique urls) to keep an entity

    Returns
    -------
    pandas.DataFrame
         pandas.DataFrame with 3 columns: `entity_name`, `locale` and `count`.
    """
    date_range = pd.date_range(
        start=start_date, end=end_date, freq="D").strftime("%Y-%m-%d").tolist()
    random.shuffle(date_range)
    entity_checked = []
    entity_stats = None
    entity_selected = None

    for idx, date in enumerate(tqdm(date_range)):
        # extract entity info for the day
        date_path = os.path.join(entity_info_path, f"entity_info_{date}.pkl")
        if os.path.exists(date_path):
            date_entity_info = pd.read_pickle(date_path)
        else:
            print(f"extracting info from ES for date: {date} ...")
            try:
                date_entity_info = get_all_entities_info_es(es_client, config, mongo_client,
                                                            query={"locale": "en"},
                                                            fields=["_id", "uri", "source_name", "snippet"
                                                                    "entity_name", "extracted_date", "locale"],
                                                            date=date)
                date_entity_info.to_pickle(date_path)
            except:
                date_entity_info = None

        # add name commonness and keep only the ones that meet the commonness condition
        if not date_entity_info is None:
            # remove entity already selected
            if not entity_selected is None:
                date_entity_info = date_entity_info[~date_entity_info.entity_name.isin(
                    entity_selected.entity_name)]

            date_entity_info >>= group_by(X.entity_id, X.entity_name) >> \
                summarize(total_uri=X.uri.unique().shape[0],
                          all_urls=[X.uri.unique().tolist()])
            entity_checked.append(date_entity_info)

        # every aggreg freq, merge the results
        if not (idx + 1) % aggreg_freq or idx == (len(date_range) - 1):
            tmp = pd.concat(entity_checked) >> group_by(X.entity_id, X.entity_name) >> \
                summarize(total_uri=X.total_uri.sum(),
                          all_urls=[X.all_urls.to_list()])

            if entity_stats is None:
                entity_stats = tmp
            else:
                entity_stats = entity_stats.append(tmp) >> group_by(X.entity_id, X.entity_name) >> \
                    summarize(total_uri=X.total_uri.sum(),
                              all_urls=[X.all_urls.to_list()])

            # clean up the urls i.e remove nested lists
            entity_stats["all_urls"] = entity_stats.apply(
                lambda row: _clean_nested_lists(row.all_urls), axis=1)

            # update: check if there are name we could add to entity_selected
            entity_candidates = entity_stats.copy()
            entity_candidates = entity_candidates[entity_candidates.total_uri >=
                                                  nbr_articles_desired]
            entity_candidates["name_com"] = entity_candidates.apply(lambda row: get_commonness_score(row.entity_name,
                                                                                                     None, None,
                                                                                                     df_names_freq,
                                                                                                     total_names),
                                    axis=1)
            entity_candidates >>= mask(X.name_com >= min_name_com_score)
            if entity_selected is None:
                entity_selected = entity_candidates.drop_duplicates(["entity_id"])
            else:
                entity_selected = entity_selected.append(
                    entity_candidates).drop_duplicates(["entity_id"])

            print(f"entity selected size:{entity_selected.shape[0]}")
            # check if we could stop now
            if entity_selected.shape[0] >= desired_total_entities:
                return entity_selected
            entity_selected.to_pickle(os.path.join(
                entity_info_path, "selected_entities.pkl"))

    return entity_selected

# Cell
def _entity_extraction_stats(entity_path: str) -> pd.DataFrame:
    _entity_name = os.path.basename(os.path.normpath(entity_path))
    return pd.DataFrame({"entity_name": _entity_name,
                         "path": entity_path,
                         "total_article": len(os.listdir(entity_path)),
                         }, index=[_entity_name])

def _save_article_local(entity_info_df: pd.DataFrame, entity_path: str) -> pd.DataFrame:
    if not os.path.exists(entity_path):
        os.mkdir(entity_path)
    for _, row in entity_info_df.iterrows():
        row.to_json(os.path.join(entity_path, row["_id"] + ".json"))

    return _entity_extraction_stats(entity_path)

# Cell
def _save_article_mongo(entity_info_df: pd.DataFrame, entity: str,
                        mongo_client: MongoClient, db: str, col: str) -> pd.DataFrame:
    entity_info_df >>= mutate(entity_name = entity, _id = entity + "_" + X["_id"])
    status = send_df_to_mongo(entity_info_df, mongo_client, db, col)
    return pd.DataFrame({"entity_name": entity,
                         "path": [entity_info_df["_id"].to_list()],
                         "total_article": entity_info_df.shape[0]},
                        index=[entity])

# Cell
def save_article_info(entity_info_df: pd.DataFrame, start_date: str, end_date: str,
                      es_client: Elasticsearch, config: dict, mongo_client: MongoClient,
                      fields: List[str] = ["domain", "date", "country",
                                           "language", "url", "title", "content"],
                      index_type: str = "articles", date_field_name: str = "tstamp",
                      nbr_articles_desired: int = 100, save_local: bool = True,
                      article_path: str = "."):
    """
    Extract and save articles related to a given entity in json format

    Parameters
    ----------
    start_date, end_date : str
        start and end date in YYYY-mm-dd format
    entity_info_df: pandas.DataFrame
        DataFrame produced by `get_entity_info()`
    es_client: ElasticSearch
        elastic search client
    mongo_client: MongoClient
        mongo client to the cluster where we want to save the data
    config : dict
        Project config dictionary
    fields: List[str]
        list of fields to extract from the 'article' database
    index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`
    date_field_name: str
        name of the date field.
    save_local: bool
        should we save the data to local disk ? if `False`, data will be saved to mongoDB
    article_path : str
        path to where to save the article extraction (json file in a directory with the entity name).
    nbr_articles_desired : int
        minimum number of articles (identified by unique urls) to keep an entity

    Returns
    -------
    pandas.DataFrame
        extraction status given info about the extraction output

    """
    # make sure url and title are in the returned fields
    if "url" not in fields:
        fields.append("url")
    if "title" not in fields:
        fields.append("title")

    # store final results here
    extraction_info = []

    for index, row in tqdm(entity_info_df.iterrows(), total=entity_info_df.shape[0]):
        entity_name_ = row.entity_name
        entity_urls = row.all_urls
        random.shuffle(entity_urls)
        query = {"url": entity_urls}
        df_entity = pd.DataFrame(get_document_es(es_client=es_client, config=config,
                                                 query=query, fields=fields,
                                                 index_type=index_type,
                                                 date_field_name=date_field_name,
                                                 start_date=start_date, end_date=end_date))
        df_entity["has_name"] = df_entity.apply(
            lambda row: entity_name_ in row.content, axis=1)
        df_entity.drop_duplicates(["url", "title"], inplace=True)
        df_entity >>= mask(X.has_name) >> head(nbr_articles_desired)

        # select where to save the data
        if save_local:
            _extraction = _save_article_local(df_entity, os.path.join(article_path, entity_name_))
        else:
            db = recursive_get(config, "mongo", "am_combiner", "db_name")
            col = recursive_get(config, "mongo", "am_combiner", "feature_test")
            _extraction = _save_article_mongo(entity_info_df, entity_name_, mongo_client, db, col)

        # update extraction status
        extraction_info.append(_extraction)

    return extraction_info

# Cell
def save_entity_data_from_es(entity_name: str, url: str,  es_client: Elasticsearch, config: dict,
                             mongo_client: MongoClient, cluster_id: Optional[int] = None,
                             mention_fields: List[str] = ["uri", "extracted_date", "entity_name", "listing_subtypes",
                                                          "entity_details.date_of_birth", "article_digest",
                                                          "article_title"],
                             article_fields: List[str] = ["domain", "date", "country",
                                                          "language", "url", "title", "content"],
                             date_field_name: str = "tstamp",
                             mention_index_type: str = "mentions", article_index_type: str = "articles",
                             save_local: bool = True, article_path: str = ".",
                             extraction_sensi: int = 500) -> List[pd.DataFrame]:
    """
    save article data related to an `entity_name` and given an associated 'url' using Elastic Search

    Parameters
    ----------
    entity_name: str
        entity-name we want to extract data for
    url: str
        url provided
    cluster_id: int
        cluster if provided by DQS
    es_client: ElasticSearch
        elastic search client
    config : dict
        Project config dictionary
    mention_fields, article_fields: List[str]
        list of fields to extract from the `mention` and the `article` database
    date_field_name: str
        name of the data field in the `article` database
    mention_index_type, article_index_type: str
        name of the index we will send the request to. Should be defined in the
        'config["elastic_search"]["indexes"]'. Currently accepted values
        are `articles` or `mentions`
    save_local: bool
        should we save the data to local disk ? if `False`, data will be saved to mongoDB
    article_path: str
        directory path where we want to save the extracted articles.
        Note that an directory per name will be created where all articles (json) files will be saved
    extraction_sensi: int
        window in days around mention extraction date where we look for data in nutch

    Returns
    -------
    List[pd.DataFrame]
        List of pandas DataFrame given the extraction status of the different matching articles found

    """
    # mandatory mention fields
    if "uri" not in mention_fields:
        mention_fields.append("uri")
    if "article_title" not in mention_fields:
        mention_fields.append("article_title")
    if "entity_name" not in mention_fields:
        mention_fields.append("entity_name")
    if "extracted_date" not in mention_fields:
        mention_fields.append("extracted_date")

    # mandatory article field
    if "url" not in article_fields:
        article_fields.append("url")
    if "title" not in article_fields:
        article_fields.append("title")

    # first query: get mentions info
    _res = get_document_es(es_client=es_client,
                           config=config,
                           query={"uri": url.strip(), "entity_name": entity_name},
                           fields=mention_fields,
                           index_type=mention_index_type)
    if not _res:
        print(f"No data found for entity:{entity_name} and url:{url}")
        return _res

    _mention_info = pd.DataFrame(_res)
    _mention_info = _mention_info[_mention_info.entity_name.isin([entity_name])].\
                    sort_values("extracted_date",  ascending=False).\
                    drop_duplicates(subset=["uri"], keep="first").\
                    drop(["_id"], axis=1)

    # second query: get article info
    extraction_info = []
    for _, row in _mention_info.iterrows():
        # format start and end date
        start_date = (pd.to_datetime(row.extracted_date) - timedelta(days=extraction_sensi)).strftime("%Y-%m-%d")
        end_date = (pd.to_datetime(row.extracted_date) + timedelta(days=extraction_sensi)).strftime("%Y-%m-%d")

        # specify query field
        if  (hasattr(row, "article_digest")) and (not row.article_digest is None) and (isinstance(row.article_digest, str)):
            query = {"digest": row.article_digest}
        else:
            query = {"url": row.uri}

        df_entity = pd.DataFrame(get_document_es(es_client=es_client, config=config,
                                                 query=query,
                                                 fields=article_fields,
                                                 index_type=article_index_type,
                                                 date_field_name=date_field_name,
                                                 start_date=start_date, end_date=end_date)
                                )

        # add some extra info
        if df_entity.shape[0]:
            if not "title" in df_entity.columns:
                df_entity["title"] = ""
            df_entity >>= mutate(ClusterID = cluster_id)
            df_entity.drop_duplicates(["url", "title"], inplace=True)
            df_entity["has_name"] = df_entity.apply(lambda row: entity_name in row.content, axis=1)

            # save the data in the appropriate type
            if save_local:
                _extraction = _save_article_local(df_entity, os.path.join(article_path, entity_name))
            else:
                db = recursive_get(config, "mongo", "am_combiner", "db_name")
                col = recursive_get(config, "mongo", "am_combiner", "validation_set")
                _extraction = _save_article_mongo(df_entity, entity_name, mongo_client, db, col)

            # update extraction status
            extraction_info.append(_extraction)

    return extraction_info

# Cell
def _extract_queue_msg(txt: str, queue_src: str = "process") -> dict:
    if queue_src == "process":
        start = txt.find("processQueueMessage=") + len("processQueueMessage=")
        end = txt.find("persistQueueMessage") - 1
    elif queue_src == "persist":
        start = txt.find("persistQueueMessage=") + len("persistQueueMessage=")
        end = len(txt)
    else:
        raise ValueError(f"queue source value not accepted:{queue_src} !")

    return json.loads(txt[start:end])

def save_entity_data_from_ae(entity_name: str, url: str, config: dict, mongo_client: MongoClient,
                             cluster_id: Optional[int] = None, queue_src = "persist",
                             as_endpoint: str = "http://web-article-extractor-dev.k8s.euw1.mi-playground-1/url",
                             save_local: bool = True, article_path: str = ".") -> List[pd.DataFrame]:
    """
    save article data related to an `entity_name` and given an associated 'url' using article-extractor api

    arameters
    ----------
    entity_name: str
        entity-name we want to extract data for;
        here only used to identify the directory path where to save the data
    url: str
        url provided
    queue_src: str
        Should the data be extracted from the `process` or the `persist` queue ?
    as_endpoint: str
        article-extractor end point url
     save_local: bool
        should we save the data to local disk ? if `False`, data will be saved to mongoDB
    article_path: str
        directory path where we want to save the extracted articles.
        Note that an directory per name will be created where all articles (json) files will be saved

    Returns
    -------
    List[pd.DataFrame]
        List of pandas DataFrame given the extraction status of the different matching articles found
    """
    # send a get request
    r = requests.get(as_endpoint, params={"url": url})
    extraction_info = []
    if r.status_code == 200:
        df_entity = pd.DataFrame(_extract_queue_msg(r.text, queue_src))
        df_entity["_id"] = create_article_id(df_entity["url"].tolist()[0],
                                             df_entity["title"].tolist()[0])
        df_entity >>= mutate(ClusterID = cluster_id)
        df_entity["has_name"] = df_entity.apply(lambda row: entity_name in row.content, axis=1)
        # save the data in the appropriate type
        if save_local:
            _extraction = _save_article_local(df_entity, os.path.join(article_path, entity_name))
        else:
            db = recursive_get(config, "mongo", "am_combiner", "db_name")
            col = recursive_get(config, "mongo", "am_combiner", "validation_set")
            _extraction = _save_article_mongo(df_entity, entity_name, mongo_client, db, col)

        extraction_info.append(_extraction)

    return extraction_info

# Cell
def _json_to_mongo(json_file: str, json_path: str, entity_name: str, mongo_client: MongoClient,
                   db: str, col: str) -> pd.DataFrame:
    _json_file_path = os.path.join(json_path, json_file)
    # read and update df
    df = pd.read_json(path_or_buf=_json_file_path, orient='index').T >> mutate(entity_name = entity_name)
    df["has_name"] = entity_name in df.content.to_list()[0]

    # try insertion in mongo
    ins_status = False
    if df["has_name"].bool():
        ins_status = send_df_to_mongo(df, mongo_client, db, col)
    else:
        print(f"problem found with file:{_json_file_path}")

    # update insertion status
    df >>= mutate(inset_status = ins_status)
    return df

def from_local_to_mongo(entity_path: str, config: dict,
                        mongo_client: MongoClient, db: str, col: str) -> pd.DataFrame:
    """
    Read extracted data from local disk and send it to mongoDB

    Parameters
    ----------
    entity_path: str
        base path passed to the extraction procedure where we have one folder per entity
    config : dict
        Project config dictionary
    mongo_client: MongoClient
        mongo client to the cluster where we want to save the data
    db,col: str
        name of the database and collection where we want to save the data

    Returns
    -------
    pd.DataFrame
        Conversion stats: information about the the data converted
    """
    # extracted entities
    entities_processed = os.listdir(entity_path)
    conversion_status = []

    for entity_name in tqdm(entities_processed):
        _entity_path = os.path.join(entity_path, entity_name)
        entity_extraction_status = pd.concat([_json_to_mongo(file, _entity_path, entity_name, mongo_client, db, col)\
                                              for file in os.listdir(_entity_path)])
        ## summarise extraction status
        entity_extraction_summary = entity_extraction_status >> group_by(X.entity_name) >>\
                                    summarize(n_files = X["_id"].unique().shape[0],
                                              n_success = X.inset_status.sum(),
                                              ids = [X["_id"].to_list()])
        conversion_status.append(entity_extraction_summary)

    # output results
    output = pd.concat(conversion_status)
    output["ids"] = output.apply(lambda row: _clean_nested_lists(row.ids), axis=1)

    return output


# Cell
def from_mongo_to_article_df(
        mongo_client: MongoClient,
        db: str,
        col: str,
        meta_data: Tuple[str, ...] ,
        keep_cols: List[str] = ["_id", "url", "content", "entity_name", "ClusterID"]
) -> pd.DataFrame:
    """
    Read extracted data from local disk and send it to mongoDB

    Parameters
    ----------
    mongo_client: MongoClient
        mongo client to the cluster where the data was saved
    db,col: str
        name of the database and collection where the data was saved
    meta_data:
        tuple of meta_data columns to retrieve
    keep_cols: List[str]
        List of columns we want to return

    Returns
    -------
    pd.DataFrame
        validation data set
    """
    return read_mongo(mongo_client=mongo_client, db=db, col=col,
                      fields={key: 1 for key in keep_cols+list(meta_data)})
