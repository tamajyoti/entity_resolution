---

title: Name commonness score


keywords: fastai
sidebar: home_sidebar



nb_path: "01_articles_data.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_articles_data.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center><strong style="font-size:40px">Articles Extraction</strong></center>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We start by setting-up some config. They are not absolutely needed but will be useful for the set of examples we will show below. 
Note that all the csv files are available in <a href="https://drive.google.com/drive/folders/1MKoyoZFa81Dep2hr9ONYujqHvevPwHoH?usp=sharing">google-drive</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Entity-Extractor">Entity Extractor<a class="anchor-link" href="#Entity-Extractor"> </a></h1><p>In this section we will extract all the entity produced over a period of time. We will only keep entities with a specific number of mentions produced and a selected name commonness. For the selected names, we will later extract the associated articles.</p>
<p>Given the poor performance of our Elastic Search database, we need to limit our interaction with it. The strategy adopted here is the following:</p>
<ol>
<li>Shuffle the dates between <code>start_date</code> and <code>end_date</code> to introduce some randomness and analyse each day separately (to avoid overloading the database)</li>
<li>Extract all entities that produced mentions for the day we are anlysing</li>
<li>Every 7 day, aggregate the results by entity name. If some of them fulfill the selection criteria, add them to the selected list of names.</li>
<li>Stop as soon as you have the desired number of entities.</li>
</ol>
<p><strong>Selection Criteria</strong></p>
<ol>
<li>name commonness larger than 1e-7</li>
<li>number of articles of at least 100</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_entity_info" class="doc_header"><code>get_entity_info</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L55" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_entity_info</code>(<strong><code>start_date</code></strong>:<code>str</code>, <strong><code>end_date</code></strong>:<code>str</code>, <strong><code>es_client</code></strong>:<code>Elasticsearch</code>, <strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>df_names_freq</code></strong>:<code>DataFrame</code>, <strong><code>total_names</code></strong>:<code>int</code>, <strong><code>locale</code></strong>:<code>str</code>=<em><code>'en'</code></em>, <strong><code>desired_total_entities</code></strong>:<code>int</code>=<em><code>1000</code></em>, <strong><code>min_name_com_score</code></strong>:<code>float</code>=<em><code>1e-07</code></em>, <strong><code>aggreg_freq</code></strong>:<code>int</code>=<em><code>15</code></em>, <strong><code>entity_info_path</code></strong>:<code>str</code>=<em><code>'.'</code></em>, <strong><code>nbr_articles_desired</code></strong>:<code>int</code>=<em><code>100</code></em>)</p>
</blockquote>
<p>Extract entity information for a given range of time</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>start_date, end_date : str
    start and end date in YYYY-mm-dd format
es_client : Elasticsearch
    elastic search client
config : dict
    Project config dictionary
mongo_client: MongoClient
    mongo-client to the cluster from where we want to extract the data
df_names_freq : pandas.DataFramce
    DataFrame needed to compute name commenness score
total_names : int
    total number of names used to compute name commenness
locale : str
    name of the language used for filtering the entity to be extracted
desired_total_entities : int
    total number of entities required for extraction
min_name_com_score: float
    minimin name commenness score to keep an entity
aggreg_freq : int
    how often do we need to aggregate the daily info extracted and check if we fulfill the extraction condition
entity_info_path : str
    path to where the daily entity extraction is saved or read from
nbr_articles_desired : int
    minimum number of articles (identified by unique urls) to keep an entity</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>pandas.DataFrame
     pandas.DataFrame with 3 columns: <code>entity_name</code>, <code>locale</code> and <code>count</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Article-Extractor">Article Extractor<a class="anchor-link" href="#Article-Extractor"> </a></h1><p>Now that we have the entities and the urls we want to extract, we just need to make an elastic search request for every entity and store the result somewhere. We have basically 2 options at this stage:</p>
<ul>
<li><em>local disk</em>: save json to disk with a specific structure. One folder per entity name and one file per article.</li>
<li><em>mongo-db</em>: save one document per entity-name per article.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Local-disk">Local disk<a class="anchor-link" href="#Local-disk"> </a></h2><p>As mentioned above, we create one folder per entity-name and one json file per article:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mongo-DB">Mongo-DB<a class="anchor-link" href="#Mongo-DB"> </a></h2><p>Saving data to mongo is even easier. We simply save one document per entity-name per article. We simply need 
to call <code>send_df_to_mongo()</code> with the appropriate data extracted and the mongo-info.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Extraction">Extraction<a class="anchor-link" href="#Extraction"> </a></h2><p>Below we define the extraction procedure. User is supposed to provide the output of <code>get_entity_info()</code> in <code>entity_df_info</code> and can decide to save the data the data locally (<code>save_local=True</code>) or send the data to mongoDB (<code>save_local=False</code>)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="save_article_info" class="doc_header"><code>save_article_info</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L201" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>save_article_info</code>(<strong><code>entity_info_df</code></strong>:<code>DataFrame</code>, <strong><code>start_date</code></strong>:<code>str</code>, <strong><code>end_date</code></strong>:<code>str</code>, <strong><code>es_client</code></strong>:<code>Elasticsearch</code>, <strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>fields</code></strong>:<code>List</code>[<code>str</code>]=<em><code>['domain', 'date', 'country', 'language', 'url', 'title', 'content']</code></em>, <strong><code>index_type</code></strong>:<code>str</code>=<em><code>'articles'</code></em>, <strong><code>date_field_name</code></strong>:<code>str</code>=<em><code>'tstamp'</code></em>, <strong><code>nbr_articles_desired</code></strong>:<code>int</code>=<em><code>100</code></em>, <strong><code>save_local</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>article_path</code></strong>:<code>str</code>=<em><code>'.'</code></em>)</p>
</blockquote>
<p>Extract and save articles related to a given entity in json format</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>start_date, end_date : str
    start and end date in YYYY-mm-dd format
entity_info_df: pandas.DataFrame
    DataFrame produced by <code>get_entity_info()</code>
es_client: ElasticSearch
    elastic search client
mongo_client: MongoClient
    mongo client to the cluster where we want to save the data
config : dict
    Project config dictionary
fields: List[str]
    list of fields to extract from the 'article' database
index_type: str
    name of the index we will send the request to. Should be defined in the
    'config["elastic_search"]["indexes"]'. Currently accepted values
    are <code>articles</code> or <code>mentions</code>
date_field_name: str
    name of the date field.
save_local: bool
    should we save the data to local disk ? if <code>False</code>, data will be saved to mongoDB
article_path : str
    path to where to save the article extraction (json file in a directory with the entity name).
nbr_articles_desired : int
    minimum number of articles (identified by unique urls) to keep an entity</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>pandas.DataFrame
    extraction status given info about the extraction output</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Feature-test-set:-extraction-example">Feature test set: extraction example<a class="anchor-link" href="#Feature-test-set:-extraction-example"> </a></h2><p>Below we show an example of how we can save the data locally. Note that we start by checking the extracted entities and only focus on the missing ones:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we check how we did so far in terms of articles by entity:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="DQS-Test-data-extractor">DQS Test data extractor<a class="anchor-link" href="#DQS-Test-data-extractor"> </a></h1><p>The DQS test set is a csv file and contains a list of names, urls (related to articles) and the cluster a name belongs to (according to annotators). In theory, we could query the article text by simply using the url provided. In practise, sending the request to Kibana <code>nutch-*</code> will likely timeout and the strategy needs to be adapted.</p>
<p>We start here by loading the DQS test set that contains a list of names and associated urls:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also before we start, we can take a look at the state of the extraction. How many names did we extract (if any) and how many are left. The following code does that:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can check here the missing / extracted statistics so far:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="From-Elastic-Search">From Elastic Search<a class="anchor-link" href="#From-Elastic-Search"> </a></h2><p>In order to narrow down the search space a bit, we will first look at <code>mention-*</code> to extract some metadata and use it later to send a more precise request to <code>nutch-*</code>. This strategy is described in the following function:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="save_entity_data_from_es" class="doc_header"><code>save_entity_data_from_es</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L282" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>save_entity_data_from_es</code>(<strong><code>entity_name</code></strong>:<code>str</code>, <strong><code>url</code></strong>:<code>str</code>, <strong><code>es_client</code></strong>:<code>Elasticsearch</code>, <strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>cluster_id</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>mention_fields</code></strong>:<code>List</code>[<code>str</code>]=<em><code>['uri', 'extracted_date', 'entity_name', 'listing_subtypes', 'entity_details.date_of_birth', 'article_digest', 'article_title']</code></em>, <strong><code>article_fields</code></strong>:<code>List</code>[<code>str</code>]=<em><code>['domain', 'date', 'country', 'language', 'url', 'title', 'content']</code></em>, <strong><code>date_field_name</code></strong>:<code>str</code>=<em><code>'tstamp'</code></em>, <strong><code>mention_index_type</code></strong>:<code>str</code>=<em><code>'mentions'</code></em>, <strong><code>article_index_type</code></strong>:<code>str</code>=<em><code>'articles'</code></em>, <strong><code>save_local</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>article_path</code></strong>:<code>str</code>=<em><code>'.'</code></em>, <strong><code>extraction_sensi</code></strong>:<code>int</code>=<em><code>500</code></em>)</p>
</blockquote>
<p>save article data related to an <code>entity_name</code> and given an associated 'url' using Elastic Search</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>entity_name: str
    entity-name we want to extract data for
url: str
    url provided
cluster_id: int
    cluster if provided by DQS
es_client: ElasticSearch
    elastic search client
config : dict
    Project config dictionary
mention_fields, article_fields: List[str]
    list of fields to extract from the <code>mention</code> and the <code>article</code> database
date_field_name: str
    name of the data field in the <code>article</code> database
mention_index_type, article_index_type: str
    name of the index we will send the request to. Should be defined in the
    'config["elastic_search"]["indexes"]'. Currently accepted values
    are <code>articles</code> or <code>mentions</code>
save_local: bool
    should we save the data to local disk ? if <code>False</code>, data will be saved to mongoDB
article_path: str
    directory path where we want to save the extracted articles.
    Note that an directory per name will be created where all articles (json) files will be saved
extraction_sensi: int
    window in days around mention extraction date where we look for data in nutch</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>List[pd.DataFrame]
    List of pandas DataFrame given the extraction status of the different matching articles found</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-article-extractor">Using <code>article-extractor</code><a class="anchor-link" href="#Using-article-extractor"> </a></h2><p>The second option is to use the <code>article-extractor</code> api. It only requires the article url and can extract its content and some useful 
metadata:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="save_entity_data_from_ae" class="doc_header"><code>save_entity_data_from_ae</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L417" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>save_entity_data_from_ae</code>(<strong><code>entity_name</code></strong>:<code>str</code>, <strong><code>url</code></strong>:<code>str</code>, <strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>cluster_id</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>queue_src</code></strong>=<em><code>'persist'</code></em>, <strong><code>as_endpoint</code></strong>:<code>str</code>=<em><code>'http://web-article-extractor-dev.k8s.euw1.mi-playground-1.uk/url'</code></em>, <strong><code>save_local</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>article_path</code></strong>:<code>str</code>=<em><code>'.'</code></em>)</p>
</blockquote>
<p>save article data related to an <code>entity_name</code> and given an associated 'url' using article-extractor api</p>
<h2 id="arameters">arameters<a class="anchor-link" href="#arameters"> </a></h2><p>entity_name: str
    entity-name we want to extract data for;
    here only used to identify the directory path where to save the data
url: str
    url provided
queue_src: str
    Should the data be extracted from the <code>process</code> or the <code>persist</code> queue ?
as_endpoint: str
    article-extractor end point url
 save_local: bool
    should we save the data to local disk ? if <code>False</code>, data will be saved to mongoDB
article_path: str
    directory path where we want to save the extracted articles.
    Note that an directory per name will be created where all articles (json) files will be saved</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>List[pd.DataFrame]
    List of pandas DataFrame given the extraction status of the different matching articles found</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Extraction-loop">Extraction loop<a class="anchor-link" href="#Extraction-loop"> </a></h2><p>This is the main loop we use to extract all the data for the dqs set (or the portion that needs to be extracted). Note that the <code>errors</code> list collects the rows where problems where found whereas the output-list returns the output of the extraction function. Note that our strategy to first query Elastic-search (ES) and if it is not successful we try article-extractor. The logic here is that ES should contain the historical data and therefore can be more reliable for news page that could have same url (example <em>bbc.co.uk/news</em>) and dynamic (changing) content:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We conclude here by investigating the names where full coverage is obtained (<code>missing &lt;= 0</code> as we may extract more articles that we need). For the incomplete names, we check how many records are missing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Convert-Data-from-Local-disk-to-mongoDB-format">Convert Data from Local disk to mongoDB format<a class="anchor-link" href="#Convert-Data-from-Local-disk-to-mongoDB-format"> </a></h1><p>These utility functions will help converting data saved on local disk to mongoDB compatible format:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="from_local_to_mongo" class="doc_header"><code>from_local_to_mongo</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L486" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>from_local_to_mongo</code>(<strong><code>entity_path</code></strong>:<code>str</code>, <strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>db</code></strong>:<code>str</code>, <strong><code>col</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Read extracted data from local disk and send it to mongoDB</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>entity_path: str
    base path passed to the extraction procedure where we have one folder per entity
config : dict
    Project config dictionary
mongo_client: MongoClient
    mongo client to the cluster where we want to save the data
db,col: str
    name of the database and collection where we want to save the data</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>pd.DataFrame
    Conversion stats: information about the the data converted</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we show how to create to save the local DQS test set to mongoDB:</p>
<p>First maybe you want to remove documents with the same ids:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Then</span><span class="p">,</span> <span class="n">you</span> <span class="n">want</span> <span class="n">to</span> <span class="n">insert</span> <span class="n">the</span> <span class="n">extracted</span> <span class="n">documents</span><span class="p">:</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the same way, we could send the feature test set to mongoDB:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="From-MongoDB-to-pandas.DataFrame">From MongoDB to <code>pandas.DataFrame</code><a class="anchor-link" href="#From-MongoDB-to-pandas.DataFrame"> </a></h1><p>Once data is on mongoDB, the next step is usually to convert the data to a <code>pandas.DataFrame</code> with all data needed to create a proper validation set:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="from_mongo_to_article_df" class="doc_header"><code>from_mongo_to_article_df</code><a href="https://git.uk/machine-learning/data-science-and-statistical-learning/am_combiner/tree/master/am_combiner/articles.py#L529" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>from_mongo_to_article_df</code>(<strong><code>config</code></strong>:<code>dict</code>, <strong><code>mongo_client</code></strong>:<code>MongoClient</code>, <strong><code>db</code></strong>:<code>str</code>, <strong><code>col</code></strong>:<code>str</code>, <strong><code>keep_cols</code></strong>:<code>List</code>[<code>str</code>]=<em><code>['_id', 'url', 'content', 'entity_name', 'ClusterID']</code></em>)</p>
</blockquote>
<p>Read extracted data from local disk and send it to mongoDB</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>config : dict
    Project config dictionary
mongo_client: MongoClient
    mongo client to the cluster where the data was saved
db,col: str
    name of the database and collection where the data was saved
keep_cols: List[str]
    List of columns we want to return</p>
<h2 id="Returns">Returns<a class="anchor-link" href="#Returns"> </a></h2><p>pd.DataFrame
    validation data set</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

